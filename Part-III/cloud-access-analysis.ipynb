{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img align=\"left\" src=\"https://earthdata.nasa.gov/img/earthdata-fb-image.jpg\" width=\"250\">\n",
    "\n",
    "## __Amazon river freshwater discharge impacts on coastal waters:__\n",
    "\n",
    "### __Hands-on tutorial of AWS in-region access of NASA Earthdata products__\n",
    "\n",
    "\n",
    "This notebook provides a basic end-to-end workflow to interact with data \"in-place\" from the NASA Earthdata Cloud, by accessing AWS s3 locations provided by [NASA Harmony](http://harmony.earthdata.nasa.gov/) outputs without the need to download data. While these outputs can be downloaded locally, the cloud offers the ability to scale compute resources to perform analyses over large areas and time spans, which is critical as data volumes continue to grow. \n",
    "\n",
    "This workflow combines search, discovery, access, reformatting, basic analyses, and plotting components presented during Part-II. Though the example we're working with in this notebook only focuses on a small time and area to account for a large number of concurrent processing requests, this workflow can be modified and scaled up to suit a larger time range and region of interest. \n",
    "\n",
    "#### Learning objectives:\n",
    "\n",
    "- Understand the Pangeo BinderHub environment used during the workshop and how to execute code within a Jupyter Notebook\n",
    "- Search for LWE (GRACE/GRACE-FO) and SMAP SST\n",
    "- Execute programmatic data access queries, plotting, and direct in-region cloud access using open source Python libraries.\n",
    "- Access data in Zarr format from Earthdata Cloud (AWS)\n",
    "- Subset both, plot and compare coincident data.\n",
    "- Identify resources, including the Earthdata Cloud Primer, for getting started with Amazon Web Services outside of the Workshop to access and work with data with a cloud environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<p float=\"left\">\n",
    "    <img src=\"https://jupyter.org/assets/main-logo.svg\" width=\"100\">\n",
    "    <img src=\"https://github.com/pangeo-data/pangeo/raw/master/docs/_static/pangeo_simple_logo.svg\" width=\"200\">\n",
    "    \n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### __Pangeo BinderHub and Project Jupyter__\n",
    "\n",
    "First, some basics on the Pangeo compute environment used during the live workshop and how to interact with Jupyter Notebooks and the Jupyter Lab interface.\n",
    "\n",
    "* [Pangeo BinderHub](https://binder.pangeo.io/): A multi-user server for interactive data analysis. This Hub is running in the AWS `us-west-2` region, which is where all Earthdata Cloud data and transformation service outputs are located. Pangeo is supported, in part, by the National Science Foundation (NSF) and the National Aeronautics and Space Administration (NASA). Google provided compute credits on Google Compute Engine. The Pangeo community promotes open, reproducible, and scalable science. We thank you for supporting this AGU Workshop.\n",
    "\n",
    "**This Hub is only supported during the live AGU workshop**. See instructions at the bottom of this notebook for how to set up your own AWS EC2 instance so that you can perform the same cloud access within your personal AWS environment. \n",
    "\n",
    "* [Jupyter Notebook](https://jupyter-notebook.readthedocs.io/en/latest/): Interactive, reproducible, open source, and exploratory browser integrated computing environment.\n",
    "* [JupyterLab](https://github.com/jupyterlab/jupyterlab): Web-based integrated IDE for computational workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Jupyter Notebook Basics__\n",
    "\n",
    "The body of a notebook is composed of cells. Each cell contains either markdown, code input, code output, or raw text. Cells can be included in any order and edited and executed at-will.\n",
    "\n",
    "**Markdown cells** - These are used to build a nicely formatted narrative around the code in the document.\n",
    "\n",
    "**Code cells** - These are used to define the computational code in the document. They come in two forms: the input cell where the user types the code to be executed, and the output cell which is the representation of the executed code.\n",
    "\n",
    "**Raw cells** - These are used when text needs to be included in raw form, without execution or transformation.\n",
    "\n",
    "#### Execute a cell or selected cells by pressing shift + enter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hello World!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collapse a cell or cell output by clicking on the blue line to the left of the cell\n",
    "\n",
    "The cell content is replaced by three dots, indicating that the cell is collapsed.\n",
    "\n",
    "#### Execute multiple cells or run the entire notebook\n",
    "Select cells with **shift + Up** or **shift + Down** and then execute selection with **shift + enter**.\n",
    "\n",
    "#### Run the whole notebook in a single step by clicking on the menu Run -> Run All Cells.\n",
    "\n",
    "See https://jupyter.readthedocs.io/en/latest/running.html for more guidance on running notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### __Import modules__\n",
    "\n",
    "The Python ecosystem is organized into modules.  A module must be imported before the contents of that modules can be used.  It is good practice to import modules in the first code cell of a notebook or at the top of your script.  Not only does this make it clear which modules are being used, but it also ensures that the code fails at the beginning because one of the modules is not installed rather half way through after crunching a load of data.\n",
    "\n",
    "For some modules, it is common practice to shorten the module names according to accepted conventions.  For example, the plotting module `matplotlib.pyplot` is shortened to `plt`.  It is best to stick to these conventions rather than making up your own short names so that people reading your code see immediately what you are doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tutorial_helper_functions as fn\n",
    "from netrc import netrc\n",
    "from platform import system\n",
    "from getpass import getpass\n",
    "from urllib import request\n",
    "from http.cookiejar import CookieJar\n",
    "from os.path import join, expanduser\n",
    "from pprint import pprint\n",
    "import intake\n",
    "import s3fs\n",
    "import rasterio\n",
    "import zarr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.animation as animation\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "import zarr\n",
    "import s3fs\n",
    "from IPython.display import HTML\n",
    "import json\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Earthdata Login__\n",
    "\n",
    "Authentication info..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Username:  amy.steiker\n",
      "Password:  ·········\n"
     ]
    }
   ],
   "source": [
    "TOKEN_DATA = (\"<token>\"\n",
    "              \"<username>%s</username>\"\n",
    "              \"<password>%s</password>\"\n",
    "              \"<client_id>PODAAC CMR Client</client_id>\"\n",
    "              \"<user_ip_address>%s</user_ip_address>\"\n",
    "              \"</token>\")\n",
    "\n",
    "\n",
    "def setup_cmr_token_auth(endpoint: str='cmr.earthdata.nasa.gov'):\n",
    "    ip = requests.get(\"https://ipinfo.io/ip\").text.strip()\n",
    "    return requests.post(\n",
    "        url=\"https://%s/legacy-services/rest/tokens\" % endpoint,\n",
    "        data=TOKEN_DATA % (input(\"Username: \"), getpass(\"Password: \"), ip),\n",
    "        headers={'Content-Type': 'application/xml', 'Accept': 'application/json'}\n",
    "    ).json()['token']['id']\n",
    "\n",
    "\n",
    "def setup_earthdata_login_auth(endpoint: str='urs.earthdata.nasa.gov'):\n",
    "    netrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n",
    "    try:\n",
    "        username, _, password = netrc(file=join(expanduser('~'), netrc_name)).authenticators(endpoint)\n",
    "    except (FileNotFoundError, TypeError):\n",
    "        print('Please provide your Earthdata Login credentials for access.')\n",
    "        print('Your info will only be passed to %s and will not be exposed in Jupyter.' % (endpoint))\n",
    "        username = input('Username: ')\n",
    "        password = getpass('Password: ')\n",
    "    manager = request.HTTPPasswordMgrWithDefaultRealm()\n",
    "    manager.add_password(None, endpoint, username, password)\n",
    "    auth = request.HTTPBasicAuthHandler(manager)\n",
    "    jar = CookieJar()\n",
    "    processor = request.HTTPCookieProcessor(jar)\n",
    "    opener = request.build_opener(auth, processor)\n",
    "    request.install_opener(opener)\n",
    "\n",
    "\n",
    "# Get your authentication token for searching restricted records in the CMR:\n",
    "_token = setup_cmr_token_auth(endpoint=\"cmr.earthdata.nasa.gov\")\n",
    "\n",
    "# Start authenticated session with URS to allow restricted data downloads:\n",
    "setup_earthdata_login_auth(endpoint=\"urs.earthdata.nasa.gov\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __CMR Search/Discovery__\n",
    "\n",
    "More info on GRACE L3 (proxy for river discharge), SMAP L3 salinity for impact assessment (maybe this goes earlier before EDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding Box spatial parameter in decimal degree 'W,S,E,N' format.\n",
    "bounding_box = '-52,-2,-43,6'\n",
    "\n",
    "# Each date in yyyy-MM-ddTHH:mm:ssZ format; date range in start,end format\n",
    "#temporal = '2019-01-01T00:00:00Z,2019-12-31T23:59:59Z'\n",
    "#temporal = '2019-02-01T00:00:00Z,2019-08-31T23:59:59Z'\n",
    "temporal = '2019-04-01T00:00:00Z,2019-04-30T23:59:59Z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_parameters = { \n",
    "    'grace': {\n",
    "        'short_name': 'TELLUS_GRAC-GRFO_MASCON_CRI_GRID_RL06_V2',\n",
    "        'provider': 'POCLOUD',\n",
    "        'bounding_box': bounding_box,\n",
    "        'temporal': temporal,\n",
    "        'token': _token},\n",
    "    'smap': {\n",
    "        'short_name': 'SMAP_RSS_L3_SSS_SMI_8DAY-RUNNINGMEAN_V4',\n",
    "        'provider': 'POCLOUD',\n",
    "        'bounding_box': bounding_box,\n",
    "        'temporal': temporal,\n",
    "        'token': _token},\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discover file number and file size \n",
    "\n",
    "Using CMR search, determine the number of files that exist over this time and area of interest, as well as the average size and total volume of those files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 files\n",
      "The total size of all files is 747.55 MB\n",
      "Found 38 files\n",
      "The total size of all files is 221.45 MB\n"
     ]
    }
   ],
   "source": [
    "search_url = \"https://cmr.earthdata.nasa.gov/search/granules\"\n",
    "output_format=\"json\"\n",
    "\n",
    "for k, v in search_parameters.items(): #fn.search_granules(search_parameters[k], _token)\n",
    "    parameters = {\n",
    "        \"scroll\": \"true\",\n",
    "        \"page_size\": 100,\n",
    "    }\n",
    "\n",
    "    response = requests.post(f\"{search_url}.{output_format}\", params=parameters, data=v)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    hits = int(response.headers['CMR-Hits'])\n",
    "    if hits > 0:\n",
    "        print(f\"Found {hits} files\")\n",
    "        results = json.loads(response.content)\n",
    "        granules = []\n",
    "        granules.extend(results['feed']['entry'])\n",
    "        granule_sizes = [float(granule['granule_size']) for granule in granules]\n",
    "        print(f\"The total size of all files is {sum(granule_sizes):.2f} MB\")\n",
    "    else:\n",
    "        print(\"Found no hits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discover data access URLs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files returned: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'URL': 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-public/TELLUS_GRAC-GRFO_MASCON_CRI_GRID_RL06_V2/GRCTellus.JPL.200204_202009.GLO.RL06M.MSCNv02CRI.cmr.json',\n",
       "  'Type': 'EXTENDED METADATA',\n",
       "  'Description': 'File to download'},\n",
       " {'URL': 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/TELLUS_GRAC-GRFO_MASCON_CRI_GRID_RL06_V2/GRCTellus.JPL.200204_202009.GLO.RL06M.MSCNv02CRI.nc',\n",
       "  'Type': 'GET DATA',\n",
       "  'Description': 'File to download'},\n",
       " {'URL': 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-public/TELLUS_GRAC-GRFO_MASCON_CRI_GRID_RL06_V2/GRCTellus.JPL.200204_202009.GLO.RL06M.MSCNv02CRI.nc.md5',\n",
       "  'Type': 'EXTENDED METADATA',\n",
       "  'Description': 'File to download'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(url=\"https://cmr.earthdata.nasa.gov/search/granules.umm_json\", \n",
    "                 params=search_parameters['grace'])\n",
    "grace_gran = r.json()\n",
    "print(\"files returned:\",grace_gran['hits'])\n",
    "grace_gran['items'][0]['umm']['RelatedUrls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/TELLUS_GRAC-GRFO_MASCON_CRI_GRID_RL06_V2/GRCTellus.JPL.200204_202009.GLO.RL06M.MSCNv02CRI.nc'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grace_url = grace_gran['items'][0]['umm']['RelatedUrls'][1]['URL']\n",
    "grace_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files returned: 38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'URL': 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-public/SMAP_RSS_L3_SSS_SMI_8DAY-RUNNINGMEAN_V4/RSS_smap_SSS_L3_8day_running_2019_087_FNL_v04.0.nc.md5',\n",
       "  'Description': 'Download RSS_smap_SSS_L3_8day_running_2019_087_FNL_v04.0.nc.md5',\n",
       "  'Type': 'EXTENDED METADATA'},\n",
       " {'URL': 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/SMAP_RSS_L3_SSS_SMI_8DAY-RUNNINGMEAN_V4/RSS_smap_SSS_L3_8day_running_2019_087_FNL_v04.0.nc',\n",
       "  'Description': 'Download RSS_smap_SSS_L3_8day_running_2019_087_FNL_v04.0.nc',\n",
       "  'Type': 'GET DATA'},\n",
       " {'URL': 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-public/SMAP_RSS_L3_SSS_SMI_8DAY-RUNNINGMEAN_V4/RSS_smap_SSS_L3_8day_running_2019_087_FNL_v04.0.cmr.json',\n",
       "  'Description': 'Download RSS_smap_SSS_L3_8day_running_2019_087_FNL_v04.0.cmr.json',\n",
       "  'Type': 'EXTENDED METADATA'},\n",
       " {'URL': 'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n",
       "  'Description': 'api endpoint to retrieve temporary credentials valid for same-region direct s3 access',\n",
       "  'Type': 'VIEW RELATED INFORMATION'},\n",
       " {'URL': 'https://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/RSS%20SMAP%20Level%203%20Sea%20Surface%20Salinity%20Standard%20Mapped%20Image%208-Day%20Running%20Mean%20V4.0%20Validated%20Dataset/granules/RSS_smap_SSS_L3_8day_running_2019_087_FNL_v04.0',\n",
       "  'Type': 'GET DATA',\n",
       "  'Subtype': 'OPENDAP DATA',\n",
       "  'Description': 'OPeNDAP request URL'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(url=\"https://cmr.earthdata.nasa.gov/search/granules.umm_json\", \n",
    "                 params=search_parameters['smap'])\n",
    "\n",
    "smap_gran = r.json()\n",
    "print(\"files returned:\",smap_gran['hits'])\n",
    "smap_gran['items'][0]['umm']['RelatedUrls']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to do a regular HTTPS download:\n",
    "    \n",
    "```python\n",
    "r = requests.get(grace_url)\n",
    "with open('tutorial7_data_GRACEFO.nc', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "!ncdump -h tutorial7_data_GRACEFO.nc\n",
    "```\n",
    "\n",
    "But we'll use the Harmony API's Zarr Reformatter service instead of downloading the entire granule. The zarr format will allow us to open and download/read just the data that we require for our Amazon Basin study area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Before we access, some background on our different AWS access methods...__\n",
    "\n",
    "- Harmony: Need access keys\n",
    "- CMR direct access: (maybe not showing this in this notebook),\n",
    "- MUR data in AWS OpenRegistry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Harmony Access__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More info on Harmony, Zarr, etc.\n",
    "\n",
    "Find collection ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1938032626-POCLOUD\n",
      "C1940468263-POCLOUD\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(url=\"https://cmr.earthdata.nasa.gov/search/collections.umm_json\", \n",
    "                 params=search_parameters['grace'])\n",
    "\n",
    "grace_coll = r.json()\n",
    "grace_coll['hits']\n",
    "grace_coll_meta = grace_coll['items'][0]['meta']\n",
    "grace_coll_id = grace_coll_meta['concept-id']\n",
    "print(grace_coll_id)\n",
    "\n",
    "r = requests.get(url=\"https://cmr.earthdata.nasa.gov/search/collections.umm_json\", \n",
    "                 params=search_parameters['smap'])\n",
    "\n",
    "smap_coll = r.json()\n",
    "smap_coll['hits']\n",
    "\n",
    "smap_coll_meta = smap_coll['items'][0]['meta']\n",
    "smap_coll_id = smap_coll_meta['concept-id']\n",
    "print(smap_coll_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMAP request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://harmony.earthdata.nasa.gov/C1940468263-POCLOUD/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset?format=application/x-zarr&subset=lat(-2:6)&subset=lon(-52:-43)&subset=time(\"2019-04-01T00:00:00Z\":\"2019-04-30T23:59:59Z\")\n"
     ]
    }
   ],
   "source": [
    "harmony_root = 'https://harmony.earthdata.nasa.gov'\n",
    "harmony_params = {\n",
    "    'collection_id': smap_coll_id,\n",
    "    'ogc-api-coverages_version': '1.0.0',\n",
    "    'variable': 'all',\n",
    "    'lat':'(-2:6)',\n",
    "    'lon':'(-52:-43)',\n",
    "    'start': '2019-04-01T00:00:00Z',\n",
    "    'stop':'2019-04-30T23:59:59Z',\n",
    "    'format': 'application/x-zarr',\n",
    "}\n",
    "\n",
    "smap_url = harmony_root+'/{collection_id}/ogc-api-coverages/{ogc-api-coverages_version}/collections/{variable}/coverage/rangeset?format={format}&subset=lat{lat}&subset=lon{lon}&subset=time(\"{start}\":\"{stop}\")'.format(**harmony_params)\n",
    "print(smap_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smap_response = request.urlopen(smap_url)\n",
    "smap_results = smap_response.read()\n",
    "smap_json = json.loads(smap_results)\n",
    "print(json.dumps(smap_json, indent=2))\n",
    "smap_jobId = smap_json['jobID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smap_job_url = f'https://harmony.earthdata.nasa.gov/jobs/{smap_jobId}'\n",
    "\n",
    "while True:\n",
    "    loop_response = request.urlopen(smap_job_url)\n",
    "    loop_results = loop_response.read()\n",
    "    job_json = json.loads(loop_results)\n",
    "    if job_json['status'] != 'running':\n",
    "        break\n",
    "    print(f\"# Job status is running. Progress is {job_json['progress']} %. Trying again.\")\n",
    "    time.sleep(10)\n",
    "\n",
    "smap_links = []\n",
    "if job_json['status'] == 'successful' and job_json['progress'] == 100:\n",
    "    print(\"# Job progress is 100%. Links to job outputs are displayed below:\")\n",
    "    smap_links = [link['href'] for link in job_json['links']]\n",
    "    display(smap_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRACE request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmony_root = 'https://harmony.earthdata.nasa.gov'\n",
    "harmony_params = {\n",
    "    'collection_id': grace_coll_id,\n",
    "    'ogc-api-coverages_version': '1.0.0',\n",
    "    'variable': 'all',\n",
    "    'lat':'(-2:6)',\n",
    "    'lon':'(-52:-43)',\n",
    "    'start': '2019-04-01T00:00:00Z',\n",
    "    'stop':'2019-04-30T23:59:59Z',\n",
    "    'format': 'application/x-zarr',\n",
    "}\n",
    "\n",
    "grace_url = harmony_root+'/{collection_id}/ogc-api-coverages/{ogc-api-coverages_version}/collections/{variable}/coverage/rangeset?format={format}&subset=lat{lat}&subset=lon{lon}&subset=time(\"{start}\":\"{stop}\")'.format(**harmony_params)\n",
    "print(grace_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grace_response = request.urlopen(grace_url)\n",
    "grace_results = grace_response.read()\n",
    "grace_json = json.loads(grace_results)\n",
    "print(json.dumps(grace_json, indent=2))\n",
    "grace_jobId = grace_json['jobID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grace_job_url = f'https://harmony.earthdata.nasa.gov/jobs/{grace_jobId}'\n",
    "\n",
    "while True:\n",
    "    loop_response = request.urlopen(grace_job_url)\n",
    "    loop_results = loop_response.read()\n",
    "    job_json = json.loads(loop_results)\n",
    "    if job_json['status'] != 'running':\n",
    "        break\n",
    "    print(f\"# Job status is running. Progress is {job_json['progress']} %. Trying again.\")\n",
    "    time.sleep(10)\n",
    "\n",
    "grace_links = []\n",
    "if job_json['status'] == 'successful' and job_json['progress'] == 100:\n",
    "    print(\"# Job progress is 100%. Links to job outputs are displayed below:\")\n",
    "    grace_links = [link['href'] for link in job_json['links']]\n",
    "    display(grace_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Access urls for the output zarr files__\n",
    "\n",
    "The new zarr dataset is staged for us in an S3 bucket. The url is the last one in the list shown above.\n",
    "\n",
    "Select the url and display below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grace_zarr_urls = grace_links[-1]\n",
    "grace_zarr_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smap_zarr_urls = smap_links[5:]\n",
    "smap_zarr_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Access credentials for the output zarr file**\n",
    "\n",
    "Credentials provided at the third and fourth urls in the list grant authenticated access to your staged S3 resources.\n",
    "\n",
    "Grab the credentials as a JSON string, load to a Python dictionary, and display their expiration date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with request.urlopen(f\"https://harmony.earthdata.nasa.gov/cloud-access\") as f:\n",
    "    creds = json.loads(f.read())\n",
    "\n",
    "creds['Expiration']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __PLACEHOLDER FOR STAC IF WE CAN GET PYSTAC WORKING__\n",
    "\n",
    "*******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### __Explore the STAC response using `intake-stac`__\n",
    "\n",
    "Each asynchronous request response includes a [STAC](https://stacspec.org/) catalog that contains spatial and temporal metadata for each output, or STAC item. These metadata fields now reflect the values of the subsetted outputs themselves, providing transformation metadata for users. The [Pangeo gallery](http://gallery.pangeo.io/repos/pangeo-data/pangeo-tutorial-gallery/intake.html) includes great guidance on how to work with stac catalogs to access cloud-hosted data in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "stac_root = 'https://harmony.earthdata.nasa.gov/stac/{jobId}/{item}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "__Open STAC Catalog from Harmony response__\n",
    "\n",
    "Several STAC items are listed, corresponding to each Zarr output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "grace_stac_cat = intake.open_stac_catalog(stac_root.format(jobId=grace_jobId,item=''),name='Harmony output')\n",
    "display(list(grace_stac_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "We can inspect the metadata of each STAC item, which includes the bounding box, coordinates, and start and end time. We'll focus on the first output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "item_0 = f'{grace_jobId}_0'\n",
    "item = grace_stac_cat[item_0]\n",
    "print(type(item))\n",
    "print(item.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Each item can be accessed from the harmony s3 staging bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "assets = list(item)\n",
    "asset = item[assets[0]]\n",
    "print(type(asset))\n",
    "print(asset.urlpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### __Access Harmony outputs directly from STAC__\n",
    "\n",
    "The Harmony output image is loaded up into an xarray data array directly from the STAC catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "smap_da = asset.to_dask()\n",
    "#smap_da.plot.imshow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open staged zarr files with *s3fs*\n",
    "\n",
    "We use the AWS `s3fs` package to get metadata about the zarr data store and list its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_fs = s3fs.S3FileSystem(\n",
    "    key=creds['AccessKeyId'],\n",
    "    secret=creds['SecretAccessKey'],\n",
    "    token=creds['SessionToken'],\n",
    "    client_kwargs={'region_name':'us-west-2'},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grace_zarr_store = zarr_fs.get_mapper(root=grace_zarr_urls, check=False)\n",
    "grace_zarr_dataset = zarr.open(grace_zarr_store)\n",
    "\n",
    "print(grace_zarr_dataset.tree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grace_zarr_dataset.lwe_thickness.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With SMAP:\n",
    "\n",
    "Trying with a single file now- how best to concatenate all of them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smap_zarr_store = zarr_fs.get_mapper(root=smap_zarr_urls[1], check=False)\n",
    "smap_zarr_dataset = zarr.open(smap_zarr_store)\n",
    "\n",
    "print(smap_zarr_dataset.tree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(smap_zarr_dataset.sss_smap.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open staged zarr file with *xarray*\n",
    "\n",
    "Here's the documentation for `xarray`'s zarr reader: http://xarray.pydata.org/en/stable/generated/xarray.open_zarr.html\n",
    "\n",
    "Open the zarr dataset and print the dataset \"header\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_GRACE = xr.open_zarr(grace_zarr_store)\n",
    "print(ds_GRACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subset by Latitude/Longitude**\n",
    "\n",
    "Once we have obtained all the data, to make processing quicker, we are going to subset datasets by latitude/longitude for the Amazon River estuary.\n",
    "\n",
    "Once we have obtained the GRACE-FO data, we should spatial subset the data to the minimal area covering the Amazon River estuary. This will reduce processing load and reduce cloud costs for the user.\n",
    "\n",
    "Make a GRACE-FO subset and display the min, max of the *lat* and *lon* variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_GRACE = ds_GRACE.sel(lat=slice(-18, 10), lon=slice(275, 330))\n",
    "print(subset_GRACE.lat.min().data, \n",
    "      subset_GRACE.lat.max().data,\n",
    "      subset_GRACE.lon.min().data,\n",
    "      subset_GRACE.lon.max().data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select the variable for Land Water Equivalent Thickness (*lwe_thickness*)**\n",
    "\n",
    "Grab the land water equivalent thickness variable from the GRACE subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lwe = subset_GRACE.lwe_thickness\n",
    "print(lwe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMAP loaded into xarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_SMAP = xr.open_zarr(smap_zarr_store)\n",
    "print(ds_SMAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots\n",
    "\n",
    "We will create an animation from sequential GRACE-FO plots over the Amazon Rainforest in the following cells. Define two functions to make the process a bit more convenient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_map(ax, pmap, ds_subset, x, y, var, t, cmap, levels, extent):\n",
    "    title = str(pd.to_datetime(ds_subset.time[t].values))\n",
    "    pmap.set_title(title, fontsize=14)\n",
    "    pmap.coastlines()\n",
    "    pmap.set_extent(extent)\n",
    "    pmap.add_feature(cartopy.feature.RIVERS)\n",
    "    variable_desired = var[t,:,:]\n",
    "    cont = pmap.contourf(x, y, variable_desired, cmap=cmap, levels=levels, zorder=1)\n",
    "    return cont\n",
    "\n",
    "def animate_ts(framenumber, ax, pmap, ds_subset, x, y, var, t, cmap, levels, extent):\n",
    "    cont = setup_map(ax, pmap, ds_subset, x, y, var, t + framenumber, cmap, levels, extent) \n",
    "    return cont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the first timestep in the JPL GRACE/GRACE-FO Mascon time series for year 2019:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a matplotlib plot object and add subplot:\n",
    "fig = plt.figure(figsize=[13,9]) \n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# Configure axes to display projected data using PlateCarree crs:\n",
    "pmap = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Get arrays of x and y to label the plot axes:\n",
    "x,y = np.meshgrid(subset_GRACE.lon, subset_GRACE.lat)                        \n",
    "\n",
    "# Set a few constants for plotting the GRACE-FO time series:\n",
    "time_start  = 168\n",
    "cmap_name   = \"bwr_r\"\n",
    "cmap_levels = np.linspace(-100., 100., 14)\n",
    "map_extent  = [-85, -30, -16, 11]\n",
    "\n",
    "# Plot the first timestep: \n",
    "cont = setup_map(ax, pmap, subset_GRACE, x, y, lwe, time_start, cmap_name, cmap_levels, map_extent)\n",
    "\n",
    "fig.colorbar(cont, ticks=cmap_levels, orientation='horizontal', label='Land Water Equivalent Thickness (cm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all the 2019 timesteps sequentially to create an animation of land water equivalent thickness for the Amazon Rainforest territories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = animation.FuncAnimation(fig, animate_ts, frames=range(0,12), fargs=(\n",
    "    ax, pmap, subset_GRACE, x, y, lwe, time_start, cmap_name, cmap_levels, map_extent\n",
    "), interval=500)\n",
    "\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User note: You will need to install 'ffmpeg' in the cmd prompt to save the .mpg to disk. Use the following command to install from the conda-forge channel:\n",
    "\n",
    "```shell\n",
    "conda install -c conda-forge ffmpeg\n",
    "```\n",
    "\n",
    "Uncomment, run the next cell to save the animation to MP4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ani.save(\"tutorial7_animation_GRACEFO.mp4\", writer=animation.FFMpegWriter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMAP Time lapse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment cell below, edit any variable names to match what was read it from harmony with xarray, and run to get the SMAP subset over region of interest (bbox).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMAP\n",
    "lat_bnds, lon_bnds = [6, -2], [-52, -43] #switched lat directions from GRACE, and longitude has positives and negatives\n",
    "ds_SMAP_subset = ds_SMAP.sel(lat=slice(*lat_bnds), lon=slice(*lon_bnds))\n",
    "ds_SMAP_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*** Note: Hit an error here - was not able to troubleshoot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot SMAP subset\n",
    "ds_SMAP_subset.sss_smap[2,:,:].plot() #at time '2', indicating June 2015\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment cell below, edit any variable names to match what was read it with harmony, and run to get the SMAP animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #A new figure window\n",
    "# fig = plt.figure(figsize=[10,8]) \n",
    "# ax = fig.add_subplot(1, 1, 1)  # specify (nrows, ncols, axnum)\n",
    "# map = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# #Necessary Variables for functions\n",
    "# extent = [-52, -43, -2, 6]                                           #lat/lon extents of map\n",
    "# x,y = np.meshgrid(ds_SMAP_subset.longitude, ds_SMAP_subset.latitude) #x, y lat/lon values for functions                                \n",
    "# levels = np.linspace(0., 45., 10)                                    #number of levels for color differentiation\n",
    "# cmap='viridis'                                                       #color scheme\n",
    "# t=0                                                                  #time to start with\n",
    "# var = ds_SMAP_subset.smap_sss                                        #variable we will be subsetting from the GRACE-FO data\n",
    "# title = str(pd.to_datetime(ds_SMAP_subset.time[t].values))           #Time of specific time step\n",
    "\n",
    "# #Set up first time step\n",
    "# cont = setup_map(ax, map, ds_SMAP_subset, x, y, var, t, cmap, levels, title, extent) \n",
    "\n",
    "# #Make a color bar\n",
    "# fig.colorbar(cont, cmap=cmap, boundaries=levels, ticks=levels, \n",
    "#              orientation='horizontal', label='Sea Surface Salinity (psu)')\n",
    "\n",
    "# #Create animation for the 2019 year (change the frame range for different time periods)\n",
    "# ani = animation.FuncAnimation(fig, animate_ts, frames=range(45,57),\n",
    "#             fargs=(ax, map, ds_SMAP_subset, x, y, var, t, cmap, levels, title, extent),  interval=400)\n",
    "\n",
    "# #Will need to install 'ffmpeg' in the cmd prompt to save the .mpg (ie. conda install -c conda-forge ffmpeg)\n",
    "# ani.save(\"SMAP_animation.mp4\", writer=animation.FFMpegWriter())\n",
    "\n",
    "# HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot (month) 2019 time series for GRACE and SMAP data\n",
    "\n",
    "For GRACE-FO, plotting lwe_thickness[107:179,34,69] indicates time, latitude, and longitude indices corresponding to the pixel for the time period 1/2019 to 12/2019 at lat/lon (-0.7, -50). The SMAP SSS is averaged over the subset lat/lon: [-2, 6], [-52, -43]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** Note: Error below - a few SMAP variables not defined. The GRACE data plotted with a modification to the commented line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot SSS and LWE thickness\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=[10,6])\n",
    "#plot LWE thickness\n",
    "\n",
    "### Error with this line, mod below:\n",
    "#ax1.plot(ds_GRACE_subset.time[167:179], ds_GRACE_subset.lwe_thickness[167:179,34,69], color = 'darkorange') \n",
    "\n",
    "ax1.plot(ds_GRACE.time[167:179], ds_GRACE.lwe_thickness[167:179,34,69], color = 'darkorange')\n",
    "\n",
    "#plot SSS on secondary axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(time_smap[45:], sss_smap_mean[45:], 'g-')\n",
    "\n",
    "ax1.set_xlabel('Date')\n",
    "ax2.set_ylabel('Sea Surface Salinity (psu)', color='g')\n",
    "ax1.set_ylabel('Land Water Equivalent Thickness (cm)', color='darkorange')\n",
    "ax2.legend(['SMAP'], loc='upper left')\n",
    "ax1.legend(['GRACE-FO'], loc='lower left')\n",
    "plt.title('Measurements Near the Amazon Estuary for 2019')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catalina Note: \n",
    "We could keep or disregard the next set of cells. If we keep, it would show that we can still read in on-prem data to add to our in-cloud analysis (in this case we're using OPeNDAP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-prem hydro data from Pre-SWOT MEaSUREs program\n",
    "\n",
    "Data from [**PRESWOT_HYDRO_GRRATS_L2_DAILY_VIRTUAL_STATION_HEIGHTS_V2**](https://podaac.jpl.nasa.gov/dataset/PRESWOT_HYDRO_GRRATS_L2_DAILY_VIRTUAL_STATION_HEIGHTS_V2) are not currently available on the cloud, but we can access via the PO.DAAC's on-prem OPeNDAP service (Hyrax) instead.\n",
    "\n",
    "<img src=\"https://podaac.jpl.nasa.gov/Podaac/thumbnails/PRESWOT_HYDRO_GRRATS_L2_DAILY_VIRTUAL_STATION_HEIGHTS_V2.jpg\" width=\"55%\">\n",
    "\n",
    "The guidebook explains the details of the Pre-SWOT MEaSUREs data: https://podaac-tools.jpl.nasa.gov/drive/files/allData/preswot_hydrology/L2/rivers/docs/GRRATS_user_handbookV2.pdf\n",
    "\n",
    "**Access URL for PO.DAAC on-prem OPeNDAP service**\n",
    "\n",
    "Identify an appropriate OPeNDAP endpoint through the following steps:\n",
    "\n",
    "1. Go to the project/mission page on the PO.DAAC portal (e.g. for Pre-SWOT MEaSUREs: https://podaac.jpl.nasa.gov/MEaSUREs-Pre-SWOT)\n",
    "\n",
    "2. Choose the dataset of interest. Go to the \"Data Access\" tab of the corresponding dataset landing page, which should like the OPeNDAP access link (for compatible datasets, e.g. for the daily river heights from virtual stations: https://podaac-opendap.jpl.nasa.gov/opendap/allData/preswot_hydrology/L2/rivers/daily/).\n",
    "\n",
    "3. Navigate to the desired NetCDF file and copy the endpoint (e.g. for our Amazon Basin use case we choose the South America file: https://opendap.jpl.nasa.gov/opendap/allData/preswot_hydrology/L2/rivers/daily/South_America_Amazon1kmdaily.nc).\n",
    "\n",
    "### Open netCDF file with *xarray*\n",
    "\n",
    "Open the netCDF dataset via OPeNDAP using *xarray*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_MEaSUREs = xr.open_dataset('https://opendap.jpl.nasa.gov/opendap/allData/preswot_hydrology/L2/rivers/daily/South_America_Amazon1kmdaily.nc')\n",
    "print(ds_MEaSUREs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our desired variable is height (meters above EGM2008 geoid) for this exercise, which can be subset by distance and time. Distance represents the distance from the river mouth, in this example, the Amazon estuary. Time is between April 8, 1993 and April 20, 2019.\n",
    "\n",
    "### Plot\n",
    "\n",
    "**Amazon River heights for March 16, 2018**\n",
    "\n",
    "Plot the river distances and associated heights on the map at time t=9069:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[13,9]) \n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.coastlines()\n",
    "ax.set_extent([-85, -30, -20, 20])\n",
    "ax.add_feature(cartopy.feature.RIVERS)\n",
    "\n",
    "plt.scatter(ds_MEaSUREs.lon, ds_MEaSUREs.lat, lw=1, c=ds_MEaSUREs.height[:,9069])\n",
    "plt.colorbar(label='Interpolated River Heights (m)')\n",
    "plt.clim(-10,100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For GRACE-FO, plotting lwe_thickness[107:179,34,69] indicates time, latitude, and longitude indices corresponding to the pixel for the time period 1/2019 to 12/2019 at lat/lon (-0.7, -50). For the 2019 year, measurements of LWE thickness followd expected patterns of high volume of water from the river output into the estuary.\n",
    "\n",
    "**2011-2019 Seasonality Plots (WIP)**\n",
    "\n",
    "For GRACE-FO, plotting lwe_thickness[107:179,34,69] indicates time, latitude, and longitude indices corresponding to the pixel for the time period 8/2011 to 12/2019 at lat/lon (-0.7, -50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot variables associated with river\n",
    "fig, ax1 = plt.subplots(figsize=[12,7])\n",
    "#plot river height\n",
    "ds_MEaSUREs.height[16,6689:9469].plot(color='darkblue')\n",
    "\n",
    "#plot LWE thickness on secondary axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(subset_GRACE.time[107:179], subset_GRACE.lwe_thickness[107:179,34,69], color = 'darkorange')\n",
    "\n",
    "ax1.set_xlabel('Date')\n",
    "ax2.set_ylabel('Land Water Equivalent Thickness (cm)', color='darkorange')\n",
    "ax1.set_ylabel('River Height (m)', color='darkblue')\n",
    "ax2.legend(['GRACE-FO'], loc='upper right')\n",
    "ax1.legend(['Pre-SWOT MEaSUREs'], loc='lower right')\n",
    "\n",
    "plt.title('Amazon Estuary, 2011-2019 Lat, Lon = (-0.7, -50)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "## __Set up for in-region access__\n",
    "\n",
    "\n",
    "__This notebook must be running within an AWS EC2 instance running in the `us-west-2` region.__\n",
    "\n",
    "For the live AGU Workshop, our BinderHub instance already takes care of steps 1 and 2, but these instructions are provided so that you can set this up in your own AWS account outside of the workshop.\n",
    "\n",
    "1. Follow tutorials 01 through 03 of the [NASA Earthdata Cloud Primer](https://earthdata.nasa.gov/learn/user-resources/webinars-and-tutorials/cloud-primer) to set up an EC2 instance within us-west-2. Ensure you are also following step 3 in the [\"Jupyter Notebooks on AWS EC2 in 12 (mostly easy) steps\"](https://medium.com/@alexjsanchez/python-3-notebooks-on-aws-ec2-in-15-mostly-easy-steps-2ec5e662c6c6) article to set the correct security group settings needed to connect your local port to your EC2’s notebook port thru SSH.\n",
    "\n",
    "2. Follow the remaining instructions in the Medium article above up until Step 11 (running Jupyter Lab). These instructions include installation of Anaconda3 (including Jupyter Lab) in your ec2 instance. Note the following updates and suggestions:\n",
    "    * Step 5: Type the following command instead of what is suggested in the article: `ssh -i \"tutorialexample.pem\" ec2-user@ec2-54-144-47-199.compute-1.amazonaws.com -L 9999:localhost:8888`. This will eliminate the need to create a ssh config file in your home directory (Step 10).\n",
    "    * As of December 2020, the most current Anaconda3 Linux distribution is: https://repo.anaconda.com/archive/Anaconda3-2020.11-Linux-x86_64.sh\n",
    "    * The Anaconda installation prompts are not the same as in the article. You will not be prompted to include Anaconda3 in your .bashrc PATH so you can skip to their step 9. Instead select \"yes\" to initialize Anaconda by running `conda init`. \n",
    "\n",
    "Before moving over to Jupyter Lab, set up your Earthdata Login authentication and Harmony access keys:\n",
    "\n",
    "3. Setup your `~/.netrc` for Earthdata Login in your ec2 instance:\n",
    "\n",
    "```\n",
    "cd ~\n",
    "touch .netrc\n",
    "echo \"machine urs.earthdata.nasa.gov login uid_goes_here password password_goes_here\" > .netrc\n",
    "chmod 0600 .netrc\n",
    "```\n",
    "\n",
    "4. Run the following in your ec2 instance terminal window to generate short-term Harmony access keys:\n",
    "\n",
    "`curl -Ln -bj https://harmony.earthdata.nasa.gov/cloud-access.sh`\n",
    "\n",
    "5. Set your environment variables based on the keys provided in step 4:\n",
    "\n",
    "`export AWS_ACCESS_KEY_ID='...\n",
    "export AWS_SECRET_ACCESS_KEY='...'\n",
    "export AWS_SESSION_TOKEN='...'\n",
    "export AWS_DEFAULT_REGION='us-west-2'`\n",
    "\n",
    "Note that these expire within 8 hours of the script generation.\n",
    "\n",
    "6. Launch jupyter lab:\n",
    "\n",
    "`jupyter lab --no-browser`\n",
    "\n",
    "Copy the URL that begins with `http://localhost:8888` into a browser window. Replace `8888` with `9999`. \n",
    "\n",
    "You should now be up and running with JupyterLab in your EC2!\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "### OLD MATERIAL FROM EARLIER DRAFT\n",
    "\n",
    "#### __On your own activity since this mimics same SMAP workflow?? Maybe have them explore using CMR or EDSC to find concurrent data first using previous tutorials...__\n",
    "\n",
    "Spatial subsetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_collection_id='C1940475563-POCLOUD'\n",
    "harmony_root = 'https://harmony.earthdata.nasa.gov'\n",
    "params = {\n",
    "    'collection_id': modis_collection_id,\n",
    "    'ogc-api-coverages_version': '1.0.0',\n",
    "    'variable': 'all',\n",
    "    'lat':'(-1.31873:2.96977)',\n",
    "    'lon':'(-53.00754:-46.39751)',\n",
    "    'start': '2020-11-15T00:00:00.000Z',\n",
    "    'stop':'2020-11-15T23:59:59.000Z'\n",
    "}\n",
    "\n",
    "modis_url = harmony_root+'/{collection_id}/ogc-api-coverages/{ogc-api-coverages_version}/collections/{variable}/coverage/rangeset?&subset=lat{lat}&subset=lon{lon}&subset=time(\"{start}\":\"{stop}\")'.format(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_results = fn.get_harmony_results(modis_url, _token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(modis_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open VIA STAC..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = modis_results['jobID']\n",
    "print(job)\n",
    "\n",
    "stac_root = 'https://harmony.earthdata.nasa.gov/stac/{jobID}/{item}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Open STAC Catalog from Harmony response__\n",
    "\n",
    "Several STAC items are listed, corresponding to each Zarr output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stac_cat = intake.open_stac_catalog(stac_root.format(jobID=job,item=''),name='Harmony output')\n",
    "display(list(stac_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the metadata of each STAC item, which includes the bounding box, coordinates, and start and end time. We'll focus on the first output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_0 = f'{job}_0'\n",
    "item = stac_cat[item_0]\n",
    "print(type(item))\n",
    "print(item.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each item can be accessed from the harmony s3 staging bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets = list(item)\n",
    "asset = item[assets[0]]\n",
    "print(type(asset))\n",
    "print(asset.urlpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Access Harmony outputs directly from STAC__\n",
    "\n",
    "The Harmony output image is loaded up into an xarray data array directly from the STAC catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_da = asset.to_dask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __MUR SST__\n",
    "\n",
    "Already reformatted to Zarr in AWS Open Registry\n",
    "\n",
    "Adapted from https://github.com/pangeo-gallery/osm2020tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $ conda install -c conda-forge zarr\n",
    "!{sys.executable} -m pip install zarr\n",
    "!{sys.executable} -m pip install aiohttp\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sst = xr.open_zarr('https://mur-sst.s3.us-west-2.amazonaws.com/zarr-v1',consolidated=True)\n",
    "\n",
    "ds_sst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst = ds_sst['analysed_sst']\n",
    "\n",
    "cond = (ds_sst.mask==1) & ((ds_sst.sea_ice_fraction<.15) | np.isnan(ds_sst.sea_ice_fraction))\n",
    "sst_masked = ds_sst['analysed_sst'].where(cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory issue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sst_day = sst_masked.sel(time='2015-10-01',lat=slice(20,65),lon=slice(-170,-110)).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sst_day.plot()\n",
    "sst_masked.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
