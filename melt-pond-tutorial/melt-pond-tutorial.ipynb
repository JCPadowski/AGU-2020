{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Coincident NASA remote sensing data over the Arctic ocean**\n",
    "### Search and subset data by geographic bounding box\n",
    "\n",
    "Melt ponds are an important feature of Arctic sea ice dynamics, leading to an decrease in sea ice albedo and other changes in heat balance. Many NASA Earthdata datasets produce variables including sea ice albedo, sea surface temperature, air temperature, and sea ice height, which can be used to better understand these dynamics. This notebook walks through the discovery, subsetting, and accessing of multiple data products coincident over an Arctic Ocean melt pond region, demonstrating how to obtain and harmonize coincident remote sensing data. \n",
    "\n",
    "As NASA Earthdata transitions to the Earthdata Cloud, it will be common for some data to exist in a traditional on-premise storage system, accessed by direct download to a local environment, while other data will have migrated to the cloud system. This need to access and customize data from both storage locations is addressed in this notebook: While one dataset's subsetting and access service utilizes the Earthdata Cloud, the data can be discovered and downloaded in the same manner as the other dataset. These Download and Service diagrams, presented in the Introduction, are highlighted in the steps below.\n",
    "\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=\"cloud_download.png\" width=\"400\" />\n",
    "  <img src=\"cloud_services.png\" width=\"400\" /> \n",
    "</p>\n",
    "\n",
    "\n",
    "### Learning objectives:\n",
    "1. Select NASA Earthdata sea ice and sea surface temperature datasets\n",
    "2. Identify data file size and availability over time and geographic bounding box\n",
    "3. Subset and download data, from both Earthdata Cloud and on-premise resources, using an Application Programmatic Interface (API)\n",
    "4. Plot and compare coincident data values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### **Explore NASA Earthdata sea ice and ocean products over a melt pond feature of interest**\n",
    "\n",
    "\n",
    "We will be working with a study area just north of Greenland in the Arctic Ocean. The OpenAltimetry application provides on-the-fly visualizations of height measurements produced by ICESat-2, where interesting features can be tagged and shared, like this melt pond observation:\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"OpenAltimetry-study-area.png\" width=\"800\" >\n",
    "\n",
    "Source: https://openaltimetry.org/data/icesat2/?start_date=2019-06-22&annoId=180\n",
    "\n",
    "\n",
    "NASA Earthdata Search enables searching, visualization, and access to data across thousands of Earth science datasets. Additional customization services are available for select datasets, including subsetting, reformatting, and reprojection. The following search results are produced from a sea ice elevation and sea surface temperature keyword search over the region of interest:\n",
    "\n",
    "[NASA Earthdata Search results from SST and sea ice height keyword selection](https://search.earthdata.nasa.gov/search?sb[0]=-62.8%2C81.7%2C-56.4%2C83&m=77.22181155531852!-91.2265556441681!2!0!0!0%2C2&fs10=Sea%20Ice%20Elevation&fsm0=Sea%20Ice&fst0=Cryosphere&fs21=Skin%20Sea%20Surface%20Temperature&fs11=Sea%20Surface%20Temperature&fsm1=Ocean%20Temperature&fst1=Oceans)\n",
    "\n",
    "There are still 15 different datasets listed(!), which highlights the breadth and variety of data cataloged by NASA Earthdata Search. We are selecting two products from this list: Level 3A sea ice height data from the ATLAS instrument onboard ICESat-2 mission, and Global Skin Sea Surface Temperature from the MODIS instrument onboard Terra. \n",
    "\n",
    "Dataset references:\n",
    "\n",
    "* [GHRSST Level 2P Global Sea Surface Skin Temperature from the Moderate Resolution Imaging Spectroradiometer (MODIS) on the NASA Terra satellite (GDS2)](https://doi.org/10.5067/GHMDT-2PJ19)\n",
    "* [ATLAS/ICESat-2 L3A Sea Ice Height](https://doi.org/10.5067/ATLAS/ATL07.003)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### **Import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #edl packages\n",
    "# import netrc\n",
    "# from urllib import request, parse\n",
    "# from http.cookiejar import CookieJar\n",
    "\n",
    "# #cmr packages\n",
    "# import socket\n",
    "\n",
    "# from gql import gql, Client\n",
    "# from gql.transport.requests import RequestsHTTPTransport\n",
    "# from pprint import pprint\n",
    "# import getpass\n",
    "# import requests\n",
    "# import json\n",
    "# import random\n",
    "# from statistics import mean\n",
    "# import pandas as pd\n",
    "# import geopandas\n",
    "# import os\n",
    "# from xml.etree import ElementTree as ET\n",
    "# import shutil\n",
    "# import time\n",
    "# import zipfile\n",
    "# import io\n",
    "# import xarray as xr\n",
    "# import h5py\n",
    "# from pathlib import Path\n",
    "# import numpy as np\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "import tutorial_helper_functions as fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___ \n",
    "\n",
    "### **Earthdata Login Authentication**\n",
    "\n",
    "An Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n",
    "\n",
    "The `setup_earthdata_login_auth` function will allow Python scripts to log into any Earthdata Login application programmatically.  To avoid being prompted for\n",
    "credentials every time you run and also allow clients such as curl to log in, you can add the following\n",
    "to a `.netrc` (`_netrc` on Windows) file in your home directory:\n",
    "\n",
    "```\n",
    "machine urs.earthdata.nasa.gov\n",
    "    login <your username>\n",
    "    password <your password>\n",
    "```\n",
    "\n",
    "Make sure that this file is only readable by the current user or you will receive an error stating\n",
    "\"netrc access too permissive.\"\n",
    "\n",
    "`$ chmod 0600 ~/.netrc` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Earthdata Login authentication for each search, subset, and access request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Username:  amy.steiker\n",
      "Password:  ·········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Earthdata Login credentials for access.\n",
      "Your info will only be passed to urs.earthdata.nasa.gov and will not be exposed in Jupyter.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Username:  amy.steiker\n",
      "Password:  ·········\n"
     ]
    }
   ],
   "source": [
    "# Get your authentication token for searching restricted records in the CMR:\n",
    "_token = fn.setup_cmr_token_auth(endpoint=\"cmr.earthdata.nasa.gov\")\n",
    "\n",
    "# Start authenticated session with URS to allow restricted data downloads:\n",
    "fn.setup_earthdata_login_auth(endpoint=\"urs.earthdata.nasa.gov\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record time and area of interest \n",
    "\n",
    "These `bounding_box` and `temporal` variables will be used for data search, subset, and access below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding Box spatial parameter in decimal degree 'W,S,E,N' format.\n",
    "bounding_box = '-62.8,81.7,-56.4,83'\n",
    "\n",
    "# Each date in yyyy-MM-ddTHH:mm:ssZ format; date range in start,end format\n",
    "temporal = '2019-06-22T00:00:00Z,2019-06-22T23:59:59Z'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore data availability using the Common Metadata Repository\n",
    "The Common Metadata Repository (CMR) is a robust metadata system that catalogs Earth Science data and associated service metadata records. CMR supports data search and discovery through an Application Programming Interface, or API, enabling reproducible data product and data file searches using a number of helpful variables, including geographic area, keyword, and time.\n",
    "\n",
    "General CMR API documentation: https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NASA Earthdata Search can be used to visualize file coverage over mulitple data sets and to access the same data you will be working with below: [insert URL with same search here]\n",
    "\n",
    "Data sets are selected by data set IDs (e.g. ATL07). In the CMR API documentation, a data set id is referred to as a \"short name\". These short names are located at the top of each NSIDC data set landing page in gray above the full title. \n",
    "\n",
    "Start by creating a nested dictionary with each data set shortname and version, as well as shared temporal range and area of interest. Data set shortnames, or IDs, as well as version numbers, are located at the top of every NSIDC landing page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_parameters = { \n",
    "    'height': {'short_name': 'ATL07','version': '003', 'provider': 'NSIDC_ECS','bounding_box': bounding_box,'temporal': temporal},\n",
    "    'sst': {'short_name': 'MODIS_T-JPL-L2P-v2019.0','version': '2019.0','provider': 'POCLOUD','bounding_box': bounding_box,'temporal': temporal},\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discover file number and file size \n",
    "\n",
    "Using CMR search, determine the number of files that exist over this time and area of interest, as well as the average size and total volume of those files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 files\n",
      "The total size of all files is 461.70 MB\n",
      "Found 11 files\n",
      "The total size of all files is 102.69 MB\n"
     ]
    }
   ],
   "source": [
    "for k, v in search_parameters.items(): fn.search_granules(search_parameters[k], _token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that subsetting, reformatting, or reprojecting can alter the size of the granules if those services are applied to your request.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Access and Subsetting**\n",
    "\n",
    "#### Determine subsetting service options\n",
    "\n",
    "We can also use CMR search to determine what transformation services, such as spatial subsetting, reformatting, reprojection, and resampling, are available for each data product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATL07 :\n",
      "{'Subset': {'SpatialSubset': {'BoundingBox': {'AllowMultipleValues': False},\n",
      "                              'Shapefile': [{'Format': 'ESRI'},\n",
      "                                            {'Format': 'KML'},\n",
      "                                            {'Format': 'GeoJSON'}]},\n",
      "            'TemporalSubset': {'AllowMultipleValues': False},\n",
      "            'VariableSubset': {'AllowMultipleValues': True}},\n",
      " 'SupportedReformattings': [{'SupportedInputFormat': 'HDF5',\n",
      "                             'SupportedOutputFormats': ['ASCII',\n",
      "                                                        'NETCDF-3',\n",
      "                                                        'NETCDF-4',\n",
      "                                                        'Shapefile']}]}\n",
      "MODIS_T-JPL-L2P-v2019.0 :\n",
      "{'SupportedReformattings': [{'SupportedInputFormat': 'HDF5',\n",
      "                             'SupportedOutputFormats': ['NETCDF-4']},\n",
      "                            {'SupportedInputFormat': 'NETCDF-4',\n",
      "                             'SupportedOutputFormats': ['NETCDF-4']}]}\n"
     ]
    }
   ],
   "source": [
    "for k, v in search_parameters.items(): \n",
    "    print(search_parameters[k]['short_name'],\":\")\n",
    "    fn.search_services(search_parameters[k], _token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, even though there are several other options for spatial area input like shapefile (see previous tutorial for this!), we'll use a simple bounding box to crop our data to that area. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Access from NSIDC outside of Earthdata Cloud\n",
    "\n",
    "ATL07, along with many other products availabe at the NSIDC DAAC, can be customized through subsetting, reformatting, and/or reprojection services prior to data access. Those options are available via a custom NSIDC API. These request options can be specified in a parameter dictionary to create the API endpoint. Insert your email into the `param_dict` to receive an email notification with your data order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://n5eil02u.ecs.nsidc.org/egi/request?short_name=ATL07&version=003&temporal=2019-06-22T00:00:00Z,2019-06-22T23:59:59Z&bounding_box=-62.8,81.7,-56.4,83&bbox=-62.8,81.7,-56.4,83&page_size=10&request_mode=async\n"
     ]
    }
   ],
   "source": [
    "#Set NSIDC data access base URL\n",
    "base_url = 'https://n5eil02u.ecs.nsidc.org/egi/request'\n",
    "\n",
    "# bounding box search and subset:\n",
    "param_dict = {'short_name': 'ATL07', \n",
    "              'version': '003', \n",
    "              'temporal': temporal, \n",
    "              'bounding_box': bounding_box, \n",
    "              'bbox': bounding_box, \n",
    "              'page_size': '10', \n",
    "              'request_mode': 'async',\n",
    "              'email': '', }\n",
    "\n",
    "#Remove blank key-value-pairs\n",
    "param_dict = {k: v for k, v in param_dict.items() if v != ''}\n",
    "\n",
    "#Convert to string\n",
    "param_string = '&'.join(\"{!s}={!r}\".format(k,v) for (k,v) in param_dict.items())\n",
    "param_string = param_string.replace(\"'\",\"\")\n",
    "\n",
    "API_request = api_request = f'{base_url}?{param_string}'\n",
    "print(API_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order status URL:  https://n5eil02u.ecs.nsidc.org/egi/request/5000000926210\n",
      "Job status is  processing . Trying again.\n",
      "Job status is  complete\n",
      "Zip download URL:  https://n5eil02u.ecs.nsidc.org/esir/5000000926210.zip\n",
      "Beginning download of zipped output...\n",
      "Download is complete.\n"
     ]
    }
   ],
   "source": [
    "fn.request_nsidc_data(API_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Access using Earthdata Cloud services\n",
    "\n",
    "The GHRSST MODIS data can be subsetted and access via [Earthdata Harmony](https://harmony.earthdata.nasa.gov/). These services are processed in the cloud, with data archived in the cloud, and outputs can be accessed by downloading to a local machine or through direct in-region access via Amazon Web Services. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# need to put this in the helper functions module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmony_root = 'https://harmony.earthdata.nasa.gov'\n",
    "\n",
    "bboxSubsetConfig = {\n",
    "    'collection_id': 'C1940475563-POCLOUD',\n",
    "    'ogc-api-coverages_version': '1.0.0',\n",
    "    'variable': 'all',\n",
    "    'lat': '(81.7:83)',\n",
    "    'lon': '(-62.8:-56.4)',\n",
    "    'time': '(\"2019-06-22T00:00:00Z\":\"2019-06-22T23:59:59Z\")'\n",
    "}\n",
    "bbox_url = harmony_root+'/{collection_id}/ogc-api-coverages/{ogc-api-coverages_version}/collections/{variable}/coverage/rangeset?&subset=lat{lat}&subset=lon{lon}&subset=time{time}'.format(**bboxSubsetConfig)\n",
    "print('Request URL', bbox_url)\n",
    "r = session.get(bbox_url)\n",
    "bbox_response = r.content\n",
    "async_json = json.loads(bbox_response)\n",
    "pprint(async_json)\n",
    "\n",
    "\n",
    "jobConfig = {\n",
    "    'jobID': async_json['jobID']\n",
    "}\n",
    "\n",
    "job_url = harmony_root+'/jobs/{jobID}'.format(**jobConfig)\n",
    "print('Job URL', job_url)\n",
    "\n",
    "job_response = session.get(job_url)\n",
    "job_results = job_response.content\n",
    "job_json = json.loads(job_results)\n",
    "\n",
    "print('Job response:')\n",
    "print()\n",
    "pprint(job_json)\n",
    "\n",
    "\n",
    "#Continue loop while request is still processing\n",
    "while job_json['status'] == 'running' and job_json['progress'] < 100: \n",
    "    print('Job status is running. Progress is ', job_json['progress'], '%. Trying again.')\n",
    "    time.sleep(10)\n",
    "    loop_response = session.get(job_url)\n",
    "    loop_results = loop_response.content\n",
    "    job_json = json.loads(loop_results)\n",
    "    if job_json['status'] == 'running':\n",
    "        continue\n",
    "\n",
    "if job_json['progress'] == 100:\n",
    "    print('Job progress is 100%. Output links printed below:')\n",
    "    links = [link for link in job_json['links'] if link.get('rel', 'data') == 'data'] #list of data links from response\n",
    "    for i in range(len(links)):\n",
    "        link_dict = links[i] \n",
    "        print(link_dict['href'])\n",
    "        filepath = link_dict['href'].split('/')[-1]\n",
    "        file_ = open(filepath, 'wb')\n",
    "        response = session.get(link_dict['href'])\n",
    "        file_.write(response.content)\n",
    "        file_.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Plotting and Comparison** \n",
    "\n",
    "File 20190622192501-JPL-L2P_GHRSST-SSTskin-MODIS_T-D-v02.0-fv01.0_subsetted.nc has data - some granules do not contain any data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = './20190622192501-JPL-L2P_GHRSST-SSTskin-MODIS_T-D-v02.0-fv01.0_subsetted.nc' # Define local filepath \n",
    "\n",
    "sst = xr.open_dataset(filepath)\n",
    "list(sst.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atl_filepath = './Outputs/processed_ATL07-01_20190622200154_13070301_003_02.h5' # Define local filepath \n",
    "\n",
    "VARIABLES = {\n",
    "    'ATL07': [\n",
    "        '/gt1l/sea_ice_segments/delta_time',\n",
    "        '/gt1l/sea_ice_segments/latitude',\n",
    "        '/gt1l/sea_ice_segments/longitude',\n",
    "        '/gt1l/sea_ice_segments/heights/height_segment_confidence',\n",
    "        '/gt1l/sea_ice_segments/heights/height_segment_height',\n",
    "        '/gt1l/sea_ice_segments/heights/height_segment_quality',\n",
    "        '/gt1l/sea_ice_segments/heights/height_segment_surface_error_est',\n",
    "        '/gt1l/sea_ice_segments/heights/height_segment_length_seg',\n",
    "        '/gt2l/sea_ice_segments/delta_time',\n",
    "        '/gt2l/sea_ice_segments/latitude',\n",
    "        '/gt2l/sea_ice_segments/longitude',\n",
    "        '/gt2l/sea_ice_segments/heights/height_segment_confidence',\n",
    "        '/gt2l/sea_ice_segments/heights/height_segment_height',\n",
    "        '/gt2l/sea_ice_segments/heights/height_segment_quality',\n",
    "        '/gt2l/sea_ice_segments/heights/height_segment_surface_error_est',\n",
    "        '/gt2l/sea_ice_segments/heights/height_segment_length_seg',\n",
    "        '/gt3l/sea_ice_segments/delta_time',\n",
    "        '/gt3l/sea_ice_segments/latitude',\n",
    "        '/gt3l/sea_ice_segments/longitude',\n",
    "        '/gt3l/sea_ice_segments/heights/height_segment_confidence',\n",
    "        '/gt3l/sea_ice_segments/heights/height_segment_height',\n",
    "        '/gt3l/sea_ice_segments/heights/height_segment_quality',\n",
    "        '/gt3l/sea_ice_segments/heights/height_segment_surface_error_est',\n",
    "        '/gt3l/sea_ice_segments/heights/height_segment_length_seg']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_icesat2_as_dataframe(filepath, VARIABLES):\n",
    "    '''\n",
    "    Load points from an ICESat-2 granule 'gt<beam>' groups as DataFrame of points. Uses VARIABLES mapping\n",
    "    to select subset of '/gt<beam>/...' variables  (Assumes these variables share dimensions)\n",
    "    Arguments:\n",
    "        filepath to ATL0# granule\n",
    "    '''\n",
    "    \n",
    "    ds = h5py.File(filepath, 'r')\n",
    "\n",
    "    # Get dataproduct name\n",
    "    dataproduct = ds.attrs['identifier_product_type'].decode()\n",
    "    # Convert variable paths to 'Path' objects for easy manipulation\n",
    "    variables = [Path(v) for v in VARIABLES[dataproduct]]\n",
    "    # Get set of beams to extract individially as dataframes combining in the end\n",
    "    beams = {list(v.parents)[-2].name for v in variables}\n",
    "    \n",
    "    dfs = []\n",
    "    for beam in beams:\n",
    "        data_dict = {}\n",
    "        beam_variables = [v for v in variables if beam in str(v)]\n",
    "        for variable in beam_variables:\n",
    "            # Use variable 'name' as column name. Beam will be specified in 'beam' column\n",
    "            column = variable.name\n",
    "            variable = str(variable)\n",
    "            try:\n",
    "                values = ds[variable][:]\n",
    "                # Convert invalid data to np.nan (only for float columns)\n",
    "                if 'float' in str(values.dtype):\n",
    "                    if 'valid_min' in ds[variable].attrs:\n",
    "                        values[values < ds[variable].attrs['valid_min']] = np.nan\n",
    "                    if 'valid_max' in ds[variable].attrs:\n",
    "                        values[values > ds[variable].attrs['valid_max']] = np.nan\n",
    "                    if '_FillValue' in ds[variable].attrs:\n",
    "                        values[values == ds[variable].attrs['_FillValue']] = np.nan\n",
    "                    \n",
    "                data_dict[column] = values\n",
    "            except KeyError:\n",
    "                print(f'Variable {variable} not found in {filepath}. Likely an empty granule.')\n",
    "                raise\n",
    "                \n",
    "        df = pd.DataFrame.from_dict(data_dict)\n",
    "        df['beam'] = beam\n",
    "        dfs.append(df)\n",
    "        \n",
    "    df = pd.concat(dfs, sort=True)\n",
    "    # Add filename column for book-keeping and reset index\n",
    "    df['filename'] = Path(filepath).name\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atl07 = load_icesat2_as_dataframe(atl_filepath, VARIABLES)\n",
    "atl07.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert sst data array to masked array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.ma as ma\n",
    "\n",
    "lon = ma.masked_invalid(sst['lon'])\n",
    "lat = ma.masked_invalid(sst['lat'])\n",
    "sst_array = ma.masked_invalid(sst['sea_surface_temperature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy\n",
    "# adapted from https://bairdlangenbrunner.github.io/python-for-climate-scientists/matplotlib/pcolormesh-grid-fix.html\n",
    "map_proj = cartopy.crs.NorthPolarStereo()\n",
    "data_proj = cartopy.crs.PlateCarree()\n",
    "#geodetic_proj = cartopy.crs.PlateCarree()\n",
    "\n",
    "fig = plt.figure(figsize=(14,6))\n",
    "ax = fig.add_subplot(111, projection=map_proj)\n",
    "ax.coastlines(color='red')\n",
    "sst_plot = ax.pcolor(lon,\\\n",
    "                      lat,\\\n",
    "                      sst_array[0],\\\n",
    "                      transform=data_proj)\n",
    "ax.set_extent([-61.5,-56.5,81.8,83])\n",
    "\n",
    "# add colorbar\n",
    "axpos = ax.get_position()\n",
    "cbar_ax = fig.add_axes([axpos.x1+0,axpos.y0,0.03,axpos.height])\n",
    "cbar = fig.colorbar(sst_plot, cax=cbar_ax)\n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "cbar.set_label('sst (kelvin)', fontsize=12)\n",
    "\n",
    "height_plot = ax.scatter(atl07.longitude,\\\n",
    "                        atl07.latitude,\\\n",
    "                        c=atl07.height_segment_height,\\\n",
    "                        vmax=1.5,\\\n",
    "                        cmap='Reds',\\\n",
    "                        alpha=0.7,\\\n",
    "                        s=1,\\\n",
    "                        transform=data_proj)\n",
    "# cbar2_ax = fig.add_axes([axpos.x1+0,axpos.y0,0.03,axpos.height])\n",
    "# cbar2 = fig.colorbar(height_plot,cax=cbar2_ax)\n",
    "# cbar2.ax.tick_params(labelsize=12)\n",
    "# cbar2.set_label('ATL07 Height (m)',fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = './20190622192501-JPL-L2P_GHRSST-SSTskin-MODIS_T-D-v02.0-fv01.0_subsetted.nc' # Define local filepath \n",
    "\n",
    "# sst = xr.open_dataset(filepath)\n",
    "# list(sst.variables)\n",
    "\n",
    "# da = xr.DataArray(sst['sea_surface_temperature'], dims=['x', 'y'], coords={'lon': (['x', 'y'], lon), 'lat': (['x', 'y'], lat)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # basic plot of atl07\n",
    "# plt.subplots(figsize=(10,5)) # set size\n",
    "# #plt.pcolormesh(sst_array[0], cmap='viridis')\n",
    "# #plt.colorbar(label='SST (kelvin)')\n",
    "# plt.scatter(atl07.longitude, atl07.latitude,\n",
    "#           c=atl07.height_segment_height, vmax=1.5,\n",
    "#           cmap='Reds', alpha=0.6, s=1)  \n",
    "# plt.colorbar(label='ATL07 Height (m)')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #basic plot of sst\n",
    "# plt.subplots(figsize=(10,5)) # set size\n",
    "# sst['sea_surface_temperature'].plot(cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Move to different notebook: Point search testing...\n",
    "\n",
    "Argo float data from [this search](https://nrlgodae1.nrlmry.navy.mil/cgi-bin/argo_select.pl?startyear=2020&startmonth=11&startday=01&endyear=2020&endmonth=11&endday=03&Nlat=75&Wlon=-80&Elon=-45&Slat=50&dac=ALL&floatid=ALL&gentype=plt&.submit=++Go++&.cgifields=endyear&.cgifields=dac&.cgifields=delayed&.cgifields=startyear&.cgifields=endmonth&.cgifields=endday&.cgifields=startday&.cgifields=startmonth&.cgifields=gentype)\n",
    "\n",
    "Pull in file with lat lon point locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"argo-float-data.csv\")\n",
    "gdf = geopandas.GeoDataFrame(df, geometry = geopandas.points_from_xy(df.Longitude, df.Latitude))\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "gdf.to_file('argo-data.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "curl -XPOST \"https://cmr.earthdata.nasa.gov/search/granules\" -F \"shapefile=@argo-data.geojson;type=application/geo+json\" -F \"collection_concept_id=C1706334166-NSIDC_ECS\" -F \"page_size=100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "search_url = \"https://cmr.earthdata.nasa.gov/search/granules\"\n",
    "files = {\"shapefile\": (\"argo-data.geojson\", open('argo-data.geojson', 'r'), \"application/geo+json\")}\n",
    "parameters = {\n",
    "    \"scroll\": \"true\",\n",
    "    \"page_size\": 100,\n",
    "    # set any search criteria here\n",
    "    \"collection_concept_id\": \"C1706334166-NSIDC_ECS\",\n",
    "}\n",
    "output_format = \"json\"\n",
    "response = requests.post(f\"{search_url}.{output_format}\", data=parameters, files=files)\n",
    "\n",
    "print(\"status:\", response.status_code)\n",
    "print(\"hits:\", response.headers[\"CMR-Hits\"])\n",
    "pprint(response.json()[\"feed\"][\"entry\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst['sea_surface_temperature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
